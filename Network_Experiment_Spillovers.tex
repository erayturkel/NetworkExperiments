\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}

\usepackage{macros/minesu_macro}

\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
\author{Mine Su Erturk, Eray Turkel}
\title{Network Experimentation Project}
\begin{document}
\maketitle

\section{Introduction}

There is a decision maker conducting experiments on a network environment. An 'experiment' in the context of our model is treating a node in the network. Each node responds to the treatment differently, depending on their (observable) characteristics.\\

The decision maker's goal is learning the optimal treatment allocation over the network by sequentially conducting local experiments on different nodes. Because of the network setting, treating one node creates spillovers on neighboring nodes.\\

Finally, the distinguishing feature of our model is that every experiment creates negative externalities on previous experiments. We model this as a cost for contaminating the treatment regime decided by a previously run experiment.\\

\subsection{Model}

There are N units (individuals, firms, locations) connected on an undirected, unweighted graph. Denote each unit on the graph by $i \in \{1, \dots, N \}$. \\

The units have observable characteristics, $\{x_1 , \dots, x_N \}$ with $x_i \in \mathbb{R}^k$. The experimenter's objective depends on the responses of the nodes to the treatment regime, denoted $\{Y_1 , \dots, Y_N\}$ with $Y \in \mathbb{R}$. The decision maker can run experiments sequentially, at time periods $t \in \{1, \dots , T\}$. Let $w_{it}$ denote the treatment assignment for node $i$ during the period of experimentation $t$, with $w_{it} \in \{0,1\}$, and denote the vector of treatment assignments at time $t$ by $w_t = [w_{1t}, \dots , w_{Nt}] \in \{0,1\}^N$. We assume that the experimentation is local, in the sense that every period, only one node can be treated. Thus for all time periods $t$, $\sum_{i \leq N} w_{it} = 1$.\\

Each node's outcome depends on their characteristics, the treatment they receive and the exposure to the treatment through their neighbors. $Y$ is assumed to be a linear function of the unit characteristics and the treatment regime. \\

Define the exposure for node $i$ at period $t$ as $e_{it}$, which is (for simplicity) assumed to be: 
$$e_{it} = \frac { \sum_{j \leq N}  \mathbf{1}(j \in nhbd(i)) w_{jt}}{ \sum_{j \leq N} \mathbf{1}(j \in nhbd(i)) }$$\\

We assume a linear response function taking the following form:

$$Y_{it}( w_{it}, e_{it}) = \beta^T x_i + \Gamma^T x_i e_{it} + \epsilon_i,$$

where $\beta, \Gamma \in \mathbb{R}^k$, and for all nodes $i$, $\epsilon_i \sim N(0, \sigma^2)$.\\

The decision maker has a prior over the value of the vector $\Gamma$ and updates their beliefs through sequential experimentation. The DM's goal is to choose a treatment assignment policy over the network based on the value of $\Gamma$. Denote the final treatment assignment for node $i$ by $w_i$, dropping the time subscript. We denote a policy by $\pi(\Gamma)$, where $\pi(\Gamma)=[w_1, \dots , w_N] \in \{0,1\}^N$. We assume that the number of nodes that can be treated during the final assignment can be at most $D$, i.e., $\sum_{i \leq N} w_i \leq D$. \\

At every period $t$, the DM's experiment creates a cost, because the experiment interacts with a previous experimenter's treatment allocation. We assume that the cost is proportional to the number of units exposed to the treatment, and once a unit is exposed, future exposures of that unit do not increase the cost any further. Define the total cost of experimentation at period $t$ by:

$$c_t = c \sum_{i \leq N} \mathbf{1}\left( e_{it} > 0 \mbox{ and } e_{is}=0,  \forall s<t \right) .$$

Hence, the total cost incurred through experimentation by the decision maker can be written as:

$$ C= \sum_{t \leq T} c_t  .$$

We assume that the DM can incur a cost of at most $\bar{C}$ during experimentation. Therefore, we require that any feasible policy $\pi$ satisfies $C \leq \bar{C} $. Then, as a function of the cost budget $\bar{C}$, we define the value of a policy as follows:

$$V(\pi, \bar{C})=\sum_{i \leq N} \Gamma^T x_i e_i(\pi), $$

where $e_i(\pi)$ denotes the exposure function at the last period, induced by the treatment allocation under policy $\pi$, and $\bar{C}$ is the cost budget. \\

The DM does not observe the vector $\Gamma$ but has an imperfect estimate of it learned through sequential experimentation, which we will call $\hat \Gamma$. We then formulate the DM's optimization problem:

\begin{align}\label{eq:dm_opti}
Max_{\hat \pi} : \sum_{i \leq N} \hat \Gamma^T x_i e_i(\hat \pi)  \quad \mbox{s.t. } 1^T \hat \pi \leq D .
\end{align}

\iffalse
This ends up being equivalent to this minimization (the equivalence to be clarified later):
%

$$ Min_{\hat \pi} E \left( Sup_\pi \left(\sum_{i \leq N} \Gamma^T x_i e_i(\pi) - c_\infty(\pi) \right) - \sum_{i \leq N} \hat \Gamma^T x_i e_i(\hat \pi) + c_\infty(\hat \pi) \right) $$
%
OR (depending on how $\hat \Gamma \rightarrow \Gamma$).

$$ Min_{\pi} E \left( Sup_\pi \left(\sum_{i \leq N} \Gamma^T x_i e_i(\pi) - c_\infty \right) - \sum_{i \leq N}  \Gamma^T x_i e_i(\pi) + \hat c_\infty \right) $$
\fi

\subsection{Last Period Deployment Problem}

Let $X_{K \times N}$ denote the matrix of observed covariates for the nodes. Let $\hat \Gamma _{1 \times K}$ be the vector of estimated coefficients.\\

Define the vector of exposure values at time t as $\bar e_t=[\mathbf{1}(e_{1t}>0), \dots, \mathbf{1}(e_{Nt}>0)]_{1 \times N}$ and let $c_t$ denote the cost vector at time t.  We can recursively write:

$$c_0 = 0_{1 \times N}$$
$$c_1 = \bar e_1$$
$$c_2 = (c_1 + e_2) - (Diag(c_1)  \bar e_2)$$
$$\vdots$$
$$c_n = (c_{n-1} + \bar e_n) - (Diag(c_{n-1})  \bar e_n), $$
%$$c_\infty = (c_{n} + \bar e_\infty) - (Diag(c_{n})  \bar e_\infty)$$\\

where $Diag(c_n)$ denotes the diagonal matrix with the entries of $c_n$ on its diagonal entries. This recursive formulation captures the notion that the cost of second and future exposures to the experiment are zero.\\

We can also define the exposure vector $e$ in terms of the adjacency matrix of the network, $A_{N \times N}$ (with self-edges), and the treatment allocation vector $\pi \in \{0,1\}^N$.\\

Representing $e$ as a vector, we have: 

$$Diag(A \mathbf{1}_{N} ) ^{-1} A\pi  = e,$$
where $\mathbf{1}_{N}$ is an $(N \times 1)$ vector of 1's.

Therefore, we can rewrite the maximization problem \eqref{eq:dm_opti} as:

$$Max_{\hat \pi}: \left(\hat \Gamma^T X Diag(A \mathbf{1}_{N} ) ^{-1} A \right) \hat \pi  $$

subject to: 

$$1^T \hat \pi \leq D $$

$$\forall i, \hat \pi_i \in \{0,1\} $$

%$$Max_{\hat \pi}: E \left( \left(\hat \Gamma^T X Diag(A \mathbf{1}_{N} ) ^{-1} A \right) \hat \pi  -  c_\infty(\hat \pi) \right)$$

\iffalse
or, we can write: %here y captures the cost structure using a linear formulation

$$Max_{\hat \pi, y_i}: E \left( \left(\hat \Gamma^T X Diag(A \mathbf{1}_{N} ) ^{-1} A \right) \hat \pi  - \sum_{i \leq N} y_i \right)$$ subject to: 

$$\forall i,  \left( Diag(A \mathbf{1}_{N} ) ^{-1} A \hat \pi \right)_i \leq y_i$$

$$\forall i, y_i, \hat \pi_i \in \{0,1\} $$
\fi

\subsection{Regret}
Let $z(\hat \Gamma)$ denote the optimal objective function value of the optimization problem \eqref{eq:dm_opti} given that the DM has estimated the coefficient vector as $\hat{\Gamma} $. Note that an oracle with full knowledge of the coefficient vector $\Gamma$ would be able to achieve $z(\Gamma)$. Thus, we can define the regret as follows:

$$R = z(\Gamma) - z(\hat{\Gamma}) $$.

\begin{thm}
The regret is upper bounded by
\begin{align}
\bP(R \leq K) \geq \ms{XXX}
\end{align}
where $K$ is a constant that depends on the adjacency matrix $A$, the allocation budget $D$, and the error in estimation $\delta$.
\end{thm}

Suppose $||c(\Gamma) - c(\hat{\Gamma})|| \leq \delta$, then $R \leq K $ where

$$ K = \delta \frac{ ||d ||  }{ \rho(d_p)} \frac{||b||}{dist(d, Dual\emptyset)- \delta } $$

where $|| \cdot ||$ refers to the 2-norm.

$||d ||  = \max \{ ||A || , ||b || , ||c ||  \} = \max \{ \sqrt{n+1}, \sqrt{n+D^2}, \ms{??} \} $

$$ K = \delta \frac{ \max \{ \sqrt{n+1}, \sqrt{n+D^2}, \ms{??} \} }{ \sqrt{n+1} } \frac{ \sqrt{n+D^2} }{dist(d, Dual\emptyset)- \delta } $$


\begin{lem}[\cite{pena2003characterization} Corollary 2.9]
For any given $A$, the distance to primal infeasibility, $dist(d_P, Pri\emptyset)$, is equal to
\begin{align*}
\rho(d_P) = \sup \{ \epsilon : ||y|| \leq \epsilon \Rightarrow y \in \{ Ax: x \geq 0, ||x|| \leq 1 \}  \},
\end{align*}
where $d_P = (A, b)$.
\end{lem}


We apply Theorem 1.1(5) in \cite{renegar1993some} to characterize the change in the optimal objective function value as a function of the perturbation to the coefficient vector $c$ of the LP. Then, we refer to Corollary 2.9 of \cite{pena2003characterization} for an equivalent characterization of the distance to the primal infeasibility region in terms of the condition number of the coefficient matrix $A$ of the LP. Finally, we replace the condition number of $A$ by computing it as the largest singular value of the matrix $A$, i.e., $\sigma_{max} (A)$. 

\begin{align*}
\rho(d_P) = || A ||_2 = \sigma_{max} (A) = \sqrt{n+1}
\end{align*}

\bibliographystyle{plain}
\bibliography{references.bib}

\end{document}